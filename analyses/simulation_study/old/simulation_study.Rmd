---
title: 'Simulation study'
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

```{r setup, message=FALSE, warning=FALSE, tidy=TRUE, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, tidy=TRUE, echo=FALSE) # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300)

WhereAmI <- "~/Dropbox/PROOF/Manuscript/mixOmics/diablo/analyses/simulation_study/"

## load libraries
library(cowplot)
library(knitr)
library(mixOmics)
library(mvtnorm)
library(NMF)

## source functions
source(paste0(WhereAmI, "simulation_study_functions.R"))
```

## DIABLO selects correlated and discriminatory variables

Briefly, three omic datasets consisting of 200 samples (split equally over two groups) and 260 variables were generated by modifying the degree of correlation and discrimination, resulting in four types of variables: 30 correlated-discriminatory (corDis) variables, 30 uncorrelated-discriminatory (unCorDis) variables, 100 correlated-nondiscriminatory (corNonDis) variables, and 100 uncorrelated-nondiscriminatory (unCorNonDis) variables. 

## Generate different types of variables and apply diablo to each type separately
* 3 datasets (effective sample size = 100; group1=100 observations, group2=100 observations)
* each dataset has four types of variables; lets explore them now
  + 30 variables that contribute to correlated & discriminatory components
  + 30 variables that contribute to correlated & nondiscriminatory components
  + 100 variables that contribute to uncorrelated & discriminatory components
  + 100 variables that contribute to uncorrelated & nondiscriminatory components

### Correlation structure for each set of simulated variables

#### corDis

```{r}
J=3; fc=3; n=50
Y <- factor(rep(c("group 1", "group 2"), each = n))
## Generate variates that contribute to the full design
rho = 0.8
sigma = matrix(rho, J, J); diag(sigma) = 1
## Variate for Group 1
bComp1 <- as.data.frame(rmvnorm(n, rep(-fc/2, J), sigma))
## Variate for Group 2
bComp2 <- as.data.frame(rmvnorm(n, rep(fc/2, J), sigma))
bComp.full <- rbind(bComp1, bComp2) 
bComp_full_irrelevant <- as.data.frame(rmvnorm(2*n, rep(0, J), sigma))
colnames(bComp.full) <- colnames(bComp_full_irrelevant) <- paste("Dataset", 1:J, sep="_")

pairPlot(bComp.full, group = Y)
```

#### corNonDis

```{r}
pairPlot(bComp_full_irrelevant, group = Y)
```

#### unCorDis

```{r}
## Generate variates that contribute to the null design
rho = 0
sigma = matrix(rho, J, J); diag(sigma) = 1
 # Variate for Group 1
bComp1 <- as.data.frame(rmvnorm(n, rep(-fc/2, J), sigma))
## Variate for Group 2
bComp2 <- as.data.frame(rmvnorm(n, rep(fc/2, J), sigma))
bComp.null <- rbind(bComp1, bComp2)  #+ matrix(rnorm(2*n*J, mean = 0, sd = noise), nc = J)
bComp_null_irrelevant <- as.data.frame(rmvnorm(2*n, rep(0, J), sigma))
colnames(bComp.null) <- colnames(bComp_null_irrelevant) <- paste("Dataset", 1:J, sep="_")

pairPlot(bComp.null, group = Y)
```

#### unCorNonDis

```{r}
pairPlot(bComp_null_irrelevant, group = Y)
```

## Simulation: vary noise and fold-change and compare with other schemes (concatenation/ensembles)

Three integrative classification methods were applied to generate multi-omic biomarkers panels of 90 variables each (30 variables from each omic dataset): a DIABLO model with either a full design (where the correlation between all pairwise combinations of datasets, as well as between each dataset and the phenotypic outcome, were maximised) or the null design (where only the correlation between each dataset and the phenotypic outcome was maximised, see Methods), a concatenation-based sPLSDA classifier which consists of naively combining all datasets into one, and an ensemble of sPLSDA classifiers where a separate sPLSDA classifier was fitted for each omics dataset and the consensus predictions were combined using a majority vote scheme. 

```{r out.width="100%"}
img1_path <- "/Users/asingh/Dropbox/PROOF/Manuscript/mixOmics/diablo/analyses/simulation_study/Figures/integrativeClassifiers.png"
include_graphics(img1_path)
```

> **Integrative prediction frameworks including multi-step approaches (concatenation, ensemble) and DIABLO to identify multi-omics molecular signatures.** Concatenation-based integration combines multiple datasets into a single large dataset, with the aim to predict a phenotype of interest. Ensemble-based classification methods construct a predictive model on each individual dataset before combining the model predictions. None of these approaches account or model relationships between datasets and thus limit our understanding of molecular interactions at multiple functional levels. DIABLO simultaneously maximizes the associations between datasets and a phenotype of interest to identify a correlated set of variables of different omics-types that are also discriminatory. The prediction is based on each omics-associated component derived from the model (see Supplementary Note). All methods presented here are data-driven approaches, which do not use any prior knowledge such as from curated biological databases (eg. protein-protein interactions).

The purpose of the simulation study was to compare DIABLO models with existing multi-step integrative classifiers with respect to the error rate and types of variables selected as part of the multi-omic biomarker panels. A secondary aim was to determine the effect of design matrix on the resulting multi-omic biomarker panels identified using DIABLO.
The concatenation, ensemble and DIABLO_null classifiers performed similarly across the various noise and fold-change thresholds. At lower noise levels (simulated using a multivariate normal distribution with mean of zero and standard deviation of 0.2 or 0.5) the DIABLO_full classifier had a slightly higher error rate compared to the other approaches, but consistently selected mostly correlated and discriminatory (corDis) variables, unlike the other integrative classifiers. All methods behaved similarly with respect to the error rate and types of variables selected at higher noise thresholds (simulated using a multivariate normal distribution with mean of zero and standard deviation of 1 or 2). This simulation highlights how the design (connection between datasets) affects the flexibility of the DIABLO model, resulting in a trade-off between discrimination or correlation. DIABLO_null focused on selecting discriminatory variables and disregarded most of the correlation between datasets (null design), whereas DIABLO_full selected highly correlated variables across all datasets. Since the variables selected by DIABLO_full reflect the correlation structure between biological compartments, we hypothesized that they might provide a balance between prediction accuracy and biological insight.


```{r simulationResults, fig.path='Figures/', dev='png', fig.height = 10, fig.width = 20}
J <- 3
fcSeq <- c(0.5, 1, 2)
noiseSeq <- c(0.2, 0.5, 1, 2)
fc.noise.grid <- expand.grid(fcSeq, noiseSeq)
colnames(fc.noise.grid) <- c("FC", "Noise")
varsList <- errList <- list()
nperms <- 5
J=3; n=100; p_relevant=30; p_irrelevant=100
folds=10; nrepeat = 1; ncomp = 1; cpus = 1

varsList_repeat <- errList_repeat <- list()
for(z in 1 : nperms){
  for(h in 1 : nrow(fc.noise.grid)){
  fc <- fc.noise.grid[h, "FC"]
  noise <- fc.noise.grid[h, "Noise"]
  
  ## Generate data given fc and noise
  genDats <- simData(fc=fc, noise=noise, J, n, p_relevant, p_irrelevant)
  data <- genDats$data
  Y <- genDats$Y
  
  ## Apply DIABLO full design
  design <- matrix(1, nrow = J, ncol = J)
  diag(design) <- 0
  keepX = list(rep(p_relevant, 1), rep(p_relevant, 1), rep(p_relevant, 1))
  rownames(design) <- colnames(design) <- names(keepX) <- names(data)
  result.full = block.splsda(X = data, Y = Y, ncomp = 1, keepX = keepX, design = design)
  diabloFullPanels <- selectVar(result.full)[1:J] %>% lapply(., function(i){ i[[1]] }) %>% unlist(.)
  cv.full <- perf(result.full, validation = "Mfold", folds = folds, nrepeat = nrepeat, progressBar = FALSE)
  
  ## Apply DIABLO null design
  design <- matrix(0, nrow = J, ncol = J)
  rownames(design) <- colnames(design) <- names(data)
  result.null = block.splsda(X = data, Y = Y, ncomp = 1, keepX = keepX, design = design)
  diabloNullPanels <- selectVar(result.null)[1:J] %>% lapply(., function(i){ i[[1]] }) %>% unlist(.)
  cv.null <- perf(result.null, validation = "Mfold", folds = folds, nrepeat = nrepeat, cpus = cpus, progressBar = FALSE)
  
  ## Concatenation
  concat <- splsda(X = do.call(cbind, data), Y = Y, keepX = rep(J*p_relevant, 1), ncomp = 1)
  concatPanels <- selectVar(concat)$name
  cv.concat <- perf(concat, validation = "Mfold", folds = folds, nrepeat = nrepeat, progressBar = FALSE)
  
  ## Ensemble
  ensemblePanels <- lapply(data, function(i){
    result <- splsda(X = i, Y = Y, keepX = rep(p_relevant, 1), ncomp = ncomp)
    selectVar(result)$name
  }) %>% unlist()
  cv.ensemble <- perf.ensemble.splsda(data, Y, keepX = rep(p_relevant, 1), ncomp = ncomp, nrepeat = nrepeat, M = folds)
  
  ## Proportion of selected variables
  panels <- list(DIABLO_Full=diabloFullPanels, DIABLO_Null=diabloNullPanels,
                 Concatenation=concatPanels, Ensemble=ensemblePanels)
  
  vars <- lapply(panels, function(i){
    as.character(sapply(strsplit(i, "\\."), function(i) i[1]))})
  dats <- lapply(panels, function(i){
    as.character(sapply(strsplit(diabloFullPanels, "_"), function(i) paste(i[-1], collapse = "_")))})
  varDat <- cbind(unlist(dats), unlist(vars)) %>% 
    as.data.frame() %>% 
    mutate(dataset=V1, varType = V2, Classifier = rep(names(dats), sapply(dats, length))) %>% 
    dplyr::select(dataset:Classifier)
  varDat$FC <- paste("FC", fc, sep = "_")
  varDat$Noise <- paste("Noise", noise, sep = "_")
  varsList[[h]] <- varDat %>% group_by(varType, Classifier, FC, Noise) %>% 
  summarise(n = n()) %>% 
  complete(varType, nesting(FC, Noise, Classifier),
    fill = list(n = 0))
  
  ## Error rates
  err <- data.frame(error = c(cv.full$WeightedVote.error.rate$max.dist["Overall.BER", ],
                            cv.null$WeightedVote.error.rate$max.dist["Overall.BER", ], 
                            cv.concat$error.rate$BER[, "max.dist"], cv.ensemble$Mean)) %>% 
         mutate(Classifier = c("DIABLO_Full", "DIABLO_Null", "Concatenation", "Ensemble"))
  err$FC <- paste("FC", fc, sep = "_")
  err$Noise <- paste("Noise", noise, sep = "_")
  errList[[h]] <- err
  }
  varsList_repeat[[z]] <- do.call(rbind, varsList)
  errList_repeat[[z]] <- do.call(rbind, errList)
}

## Error rates
errorRates <- do.call(rbind, errList_repeat)  %>% 
    mutate(Classifier = factor(Classifier, 
    levels = c("DIABLO_Full", "DIABLO_Null", "Concatenation", "Ensemble"))) %>% 
  group_by(Classifier, FC, Noise) %>% 
  summarise(Mean = mean(error), SD = sd(error)) %>% 
  ggplot(aes(x = Classifier, y = Mean)) +
  geom_bar(stat = "identity") + 
  geom_errorbar(aes(ymin = Mean-SD, ymax = Mean+SD)) +
  facet_grid(FC ~ Noise) +
  customTheme(sizeStripFont = 25, xAngle = -45, hjust = 0, vjust = 1, 
              xSize = 15, ySize = 15, xAxisSize = 15, yAxisSize = 15) +
  ylab("Mean±SD of error rate \n 10-fold cross-validation over 50 simulations") +
  xlab("Integrative Classifiers") +
  geom_hline(yintercept = 0.50, linetype = "dashed")


### Selected variables
selectedVars <- do.call(rbind, varsList_repeat) %>% ungroup %>% 
    mutate(Classifier = factor(Classifier, 
    levels = c("DIABLO_Full", "DIABLO_Null", "Concatenation", "Ensemble"))) %>% 
  group_by(Classifier, FC, Noise, varType) %>% 
  summarise(Mean = mean(n), SD = sd(n)) %>% 
  ggplot(aes(x = Classifier, y = Mean, color = varType, fill = varType)) +
  geom_bar(stat = "identity") + 
  #geom_errorbar(aes(ymin = Mean, ymax = Mean+SD), position="dodge") +
  facet_grid(FC ~ Noise) +
  customTheme(sizeStripFont = 25, xAngle = -45, hjust = 0, vjust = 1, 
              xSize = 15, ySize = 15, xAxisSize = 15, yAxisSize = 15) +
  ylab("Average number of selected variables \n across 3 datasets over 50 simulations") +
  geom_hline(yintercept = 90, linetype = "dashed") + xlab("Integrative Classifiers") +
  scale_y_continuous(breaks=c(30,60,90))

plot_grid(errorRates, selectedVars, labels = c("a", "b"), label_size = 30)
```

> **Simulation study: performance assessment and benchmarking.** Simulated datasets included different types of variables: correlated & discriminatory (corDis); uncorrelated & discriminatory (unCorDis); correlated & nondiscriminatory (corNonDis) and uncorrelated & nondiscriminatory (unCorNonDis) for different fold-changes between sample groups and different noise levels (see Supplementary Note). Integrative classifiers included DIABLO with either the full or null design, concatenation and ensemble-based sPLSDA classifiers and were all trained to select 90 variables across three multi-omics datasets. a) Classification error rates (10-fold cross-validation averaged over 50 simulations). Dashed line indicates a random performance (error rate = 50%). All methods perform similarly with the exception of DIABLO_full which has a higher error rate. b) Number of variables selected according to their type. DIABLO_full selected mainly variables that were correlated & discriminatory (corDis, red), whereas the other methods selected an equal number of correlated or uncorrelated discriminatory variables (corDis and unCorDis, red and blue).